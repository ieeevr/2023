id,title,,,abstract,authors
J1034,Evidence of Racial Bias Using Immersive Virtual Reality: Analysis of Head and Hand Motions During Shooting Decisions,,,"Shooter bias is the tendency to more quickly shoot at unarmed Black suspects compared to unarmed White suspects. This research investigates the efficacy of shooter bias simulation studies in an immersive virtual scenario and presents results from a user study (N=99) investigating shooter and racial bias. More nuanced head and hand motion analysis was able to predict participantsí racial shooting accuracy and implicit racism scores. Discussion of how these nuanced measures can be used for detecting behavior changes for body-swap illusions, and implications of this work related to racial justice and police brutality are discussed.",Tabitha C. Peck: Davidson College; Jessica J Good: Davidson College; Katharina Seitz: Davidson College
J1035,Comparing and Combining Virtual Hand and Virtual Ray Pointer Interactions for Data Manipulation in Immersive Analytics,,,"In this work, we evaluate two standard interaction techniques for Immersive Analytics, virtual hands and virtual ray pointers, together with a third option, which seamlessly integrates both without explicit mode switching. Our findings indicate that both virtual hands and ray-casting result in similar performance, workload ratings, and even interactivity patterns. Yet, the mixed mode significantly reduced completion times by 23% for the most demanding task, at the cost of a 5% decrease in overall success rates. Also, most participants clearly preferred being able to choose the best interaction technique for each low-level task.",Jorge Wagner: Federal University of Rio Grande do Sul; Wolfgang Stuerzlinger: Simon Fraser University; Luciana Nedel: Federal University of Rio Grande do Sul (UFRGS)
J1036,Group Navigation for Guided Tours in Distributed Virtual Environments,,,"Group navigation can be an invaluable tool for performing guided tours in distributed virtual environments. We derived that group navigation techniques should be comprehensible for the guide and attendees, assist in obstacle avoidance, and allow the creation of meaningful spatial arrangements. To meet these requirements, we developed a group navigation technique based on short-distance teleportation and performed an initial usability evaluation. After navigating with groups of up to 10 users through a virtual museum, participants indicated that our technique is easy to learn for guides, comprehensible also for attendees, non-nauseating for both roles, and therefore well-suited for performing guided tours.",Tim Weissker: Bauhaus-Universitaet Weimar; Bernd Froehlich: Bauhaus-Universität Weimar
J1044,ARC: Alignment-based Redirection Controller for Redirected Walking in Complex Environments,,,"We present a novel redirected walking controller based on alignment that allows the user to explore large, complex virtual environments, while minimizing the number of collisions in the physical environment. Our alignment-based redirection controller (ARC) steers the user such that their proximity to obstacles in the physical environment matches the proximity to obstacles in the virtual environment as closely as possible. We introduce a new metric to measure the relative environment complexity and characterize the difference in navigational complexity between the physical and virtual environments. ARC significantly outperforms state-of-the-art controllers and works in arbitrary environments without any user input.","Niall L. Williams: University of Maryland, College Park; Aniket Bera: University of Maryland at College Park; Dinesh Manocha: University of Maryland"
J1050,"Lenslet VR: Thin, Flat and Wide-FOV Virtual Reality Display Using Fresnel Lens and Lenslet Array",,,"We propose a new thin and flat VR display design using a Fresnel lenslet array, a Fresnel lens, and a polarization-based optical folding technique. The proposed optical system has a wide FOV of 102˚×102˚, a wide eye-box of 8.8 mm, and an ergonomic eye-relief of 20 mm while having a few millimeters of system thickness. We demonstrate an 8.8-mm-thick VR glasses prototype and experimentally verify that the proposed VR display system has the expected performance while having a glasses-like form factor.",Kiseung Bang: Seoul National University; Youngjin Jo: Seoul National University; Minseok Chae: Seoul National University; Byoungho Lee: Seoul National University
J1068,A privacy-preserving approach to streaming eye-tracking data,,,"Eye-tracking technology is being increasingly integrated into mixed reality devices. Although critical applications are being enabled, there are significant possibilities for violating user privacy expectations. We propose a framework that incorporates gatekeeping via the design of the application programming interface and via software-implemented privacy mechanisms. Our results indicate that these mechanisms can reduce the rate of identification from as much as 85% to as low as 30%. The impact of introducing these mechanisms is less than 1.5⁰error in in gaze prediction. Our approach is the first to support privacy-by-design in the flow of eye-tracking data within mixed reality use cases.",Brendan David-John: University of Florida; Diane Hosfelt: Mozilla; Kevin Butler: University of Florida; Eakta JAIN: University of Florida
J1077,"SPinPong - Virtual Reality Table Tennis Skill Acquisition using Visual, Haptic and Temporal Cues",,,"Returning strong spin shots in table tennis needs to judge the spin and decide the racket pose within a second, which is very difficult. Therefore, we introduce an intuitive training system to acquire this specific skill using different information in VR, such as visual cues, haptic devices, and distorting the time. In the study, we first obtain some insights about augmentation for training spin shots. The training system is improved based on a VR game by adding new conditions using different cues. Finally, we performed a detailed experiment to study the effect of each condition both quantitatively and qualitatively to provide an entrance for VR table tennis training.",Erwin Wu: Tokyo Institute of Technology; Mitski Piekenbrock: Tokyo Institute of Technology; Takuto Nakamura: Tokyo Institute of Technology; Hideki Koike: Tokyo Institute of Technology
J1093,"Event-Based Near-Eye Gaze Tracking Beyond 10,000 Hz",,,"We developed a hybrid frame-event-based near-eye graze tracking system offering updates beyond 10,000 Hz with an accuracy of 0.45-1.75 degrees for fields of view from 45-95 degrees. We hope this new technology enables a new generation of efficient, ultra-low-latency gaze-contingent rendering and display techniques for VR/AR.","Anastasios Angelopoulos: University of California, Berkeley; Julien N.P. Martel: Stanford University; Amit Pal Singh Kohli: University of California, Berkeley; Jorg Conradt: KTH Stockholm; Gordon Wetzstein: Stanford University"
J1096,Real-Time Omnidirectional Stereo Rendering: Generating 360° Surround-View Panoramic Images for Comfortable Immersive Viewing,,,"Omnidirectional stereo images (surround-view panoramas) can provide an immersive experience for pre-generated content, but typically suffer from binocular misalignment in the peripheral vision as a user's view direction approaches the north / south pole of the projection sphere. This paper presents a real-time geometry-based approach for omnidirectional stereo rendering that fits into the standard rendering pipeline. Our approach includes tunable parameters that enable pole merging -- a reduction in the stereo effect near the poles that can minimize binocular misalignment. Results from a user study indicate that pole merging reduces visual fatigue and discomfort associated with binocular misalignment without inhibiting depth perception.",Thomas Marrinan: University of St. Thomas; Michael E. Papka: Argonne National Laboratory
J1119,GestOnHMD: Enabling Gesture-based Interaction on the Surface of Low-cost VR Head-Mounted Display,,,"We present GestOnHMD, a gesture-based interaction technique that leverages the stereo microphones in a commodity smartphone to detect scratching gestures on three surfaces on a mobile VR headset. We first proposed a set of user-defined gestures on different surfaces of the Google Cardboard, with the consideration on user preferences and signal detectability. We then constructed a data set containing the acoustic signals of 18 users performing these on-surface gestures. Using this data set, we trained the three-step deep-learning classification pipeline for gesture detection and recognition. Lastly, we conducted a series of online participatory-design sessions to collect a set of user-defined gesture-referent mappings that could potentially benefit from GestOnHMD.",Taizhou Chen: City University of Hong Kong; Lantian Xu: City University of Hong Kong; Xianshan Xu: City University of Hong Kong; Kening Zhu: City University of Hong Kong
J1138,The Influence of Avatar Representation on Interpersonal Communication in Virtual Social Environments,,,"Current avatar representations used in immersive VR applications lack features that may support natural behaviors and effective communication among individuals. This work investigates the impact of the visual and nonverbal cues afforded by three different types of avatar representations in social context tasks. The avatar types we compared are: No_Avatar (HMD and controllers only), Scanned_Avatar (wearing an HMD), and Real_Avatar (video-see-through). We used subjective and objective measures to assess the quality of interpersonal communication. The findings provide novel insight into how a user’s experience in a social VR scenario is affected by the type of avatar representation provided.",Sahar Aseeri: University of Minnesota; Victoria Interrante: University of Minnesota
J1160,Tactile Perceptual Thresholds of Electrovibration in VR,,,"To produce high-fidelity haptic feedback in virtual environments, various haptic devices and tactile rendering methods have been explored, and perception deviation between a virtual environment and a real environment has been investigated. However, the tactile sensitivity for touch perception in a virtual environment has not been fully studied. This paper investigates users’ perceptual thresholds when they are immersed in a virtual environment by utilizing electrovibration tactile feedback and by generating tactile stimuli with different waveform, frequency and amplitude characteristics. We believe that our work on tactile perceptual thresholds can promote future research that focuses on creating a favorable haptic experience for VR applications.",Lu Zhao: Beijing Institute of Technology; Yue Liu: Beijing Institute of Technology; Weitao Song: Beijing Institute of Technology
J1169,Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality,,,"For immersive proxy-based haptics in virtual reality, the challenges of similarity and colocation must be solved. To do so, we propose to combine the techniques of Dynamic Passive Haptic Feedback and Haptic Retargeting, which both have been shown individually to be successful in tackling these challenges, but up to now, have not ever been studied in combination. In two proof-of-concept experiments focused on the perception of virtual weight distribution inside a stick, we validate that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.","André Zenner: Saarland University, Saarland Informatics Campus; Kristin Ullmann: Saarland University, Saarland Informatics Campus; Antonio Krüger: Saarland University, Saarland Informatics Campus"
J1171,A Log-Rectilinear Transformation for Foveated 360-degree Video Streaming,,,"With the rapidly increasing resolutions of 360° cameras, head-mounted displays, and live-streaming services, streaming high-resolution panoramic videos over limited-bandwidth networks is becoming a critical challenge. Foveated video streaming can address this rising challenge in the context of eye-tracking-equipped virtual reality head-mounted displays. However, conventional log-polar foveated rendering suffers from a number of visual artifacts such as aliasing and flickering. In this paper, we introduce a new log-rectilinear transformation that incorporates summed-area table filtering and off-the-shelf video codecs to enable foveated streaming of 360° videos suitable for VR headsets with built-in eye-tracking.",David Li: University Of Maryland; Ruofei Du: Google; Adharsh Babu: University of Maryland College Park; Camelia D. Brumar: University of Maryland; Amitabh Varshney: University of Maryland College Park
J1177,Text Entry in Virtual Environments using Speech and a Midair Keyboard,,,"We investigate text input in virtual reality using hand-tracking and speech. Our system visualizes users' hands, allowing typing on an auto-correcting midair keyboard. It supports speaking a sentence and correcting errors by selecting alternative words proposed by a speech recognizer. Using only the keyboard, participants wrote at 11 words-per-minute at a 1.2% error rate. Speaking and correcting was faster and more accurate at 28 words-per-minute and a 0.5% error rate. Participants achieved this despite half of sentences containing an out-of-vocabulary word. For sentences with only in-vocabulary words, performance was even faster at 36 words-per-minute.",Jiban Krishna Adhikary: Michigan Technological University; Keith Vertanen: Michigan Technological University
J1185,Beaming Displays,,,"We present the beaming displays, a new type of near-eye display system that uses a projector and an all passive wearable headset. We modify an off-the-shelf projector with additional lenses. We install such a projector to the environment to beam images from a distance to a passive wearable headset. The beaming projection system tracks the current position of a wearable headset to project distortion-free images with correct perspectives. In our system, a wearable headset guides the beamed images to a user's retina, which are then perceived as an augmented scene within a user's field of view.",Yuta Itoh: Tokyo Institute of Technology; Takumi Kaminokado: Tokyo Institute of Technology; Kaan Akşit: University College London
J1192,Floor-vibration VR: Mitigating Cybersickness Using Whole-body Tactile Stimuli in Highly Realistic Vehicle Driving Experiences,,,"Starting from sensory conflict theory, we posit that if a vertical-vibrating floor delivers vestibular stimuli that minimally match the vibration characteristics of a scenario, the conflict between the visual and vestibular senses will be reduced and, thus, the incidence and/or severity of cybersickness will also be reduced. We implemented a realistic off-road vehicle simulator and programmed the floor to generate vibrations similar to those experienced in real off-road vehicle driving. Comparing data (presence, SSQ, self-rated-discomfort-levels, HR, GSR, pupil-size) between participants in the Vibration-group to the No-Vibration-group, we found that the vibration significantly reduced measures (SSQ and GSR) of cybersickness.",Sungchul Jung: University of Canterbury; Richard Chen Li: University of Canterbury; Ryan Douglas McKee: University of Canterbury; Mary C. Whitton: UNC Chapel Hill; Robert W. Lindeman: University of Canterbury
J1200,FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments,,,"Human visual attention in immersive virtual reality (VR) is key for many important applications, such as gaze-contingent rendering and gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.  Then we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors. Based on the analysis, we propose FixationNet -- a novel learning-based model to forecast users' eye fixations in the near future in VR.",Zhiming Hu: Peking University; Andreas Bulling: University of Stuttgart; Sheng Li: Peking University; Guoping Wang: Computer Science and Technology
J1202,Quality of Service Impact on Edge Physics Simulations for VR,,,"Mobile HMDs sacrifice performance for ergonomics and power efficiency, meaning mobile applications must also sacrifice complexity, or offload it. A common approach is render streaming, but latency requirements make VR a particularly challenging use case. In our paper we re-examine scene-graph streaming, but in the context of edge-computing. With 'edge-physics', latency sensitive loops run locally, tasks that hit the power-wall of mobile CPUs are off-loaded, while improving GPUs are leveraged to maximum effect. We investigate the potential of edge physics by implementing a prototype and evaluating its response to varying quality of service.",Sebastian J Friston: University College London; Elias J Griffith: University of Liverpool; David Swapp: University College London; Iheanyi Caleb Irondi: University of Liverpool; Fred P. M. Jjunju: UNiversity of Liverpool; Ryan J Ward: University of Liverpool; Alan Marshall: University of Liverpool; Anthony Steed: University College London
J1222,Hands-free interaction in immersive virtual reality: A systematic review,,,"Hands are the most important tool to interact with immersive virtual environments and they should be available to perform the most critical tasks. For example, a surgeon in VR should keep his/her hands on the instruments and be able to do secondary tasks without performing a disruptive event. In this situation, one can observe that hands are not available for interaction. This systematic review surveys the literature and provides a quantitative and qualitative analysis of which hands-free interfaces are used, the performed interaction tasks, what metrics are used for evaluation, the results of such evaluations, and research opportunities.",Pedro Monteiro: INESC TEC; Guilherme Gonçalves: INESC TEC; Hugo Coelho: INESC TEC; Miguel Melo: INESC TEC; Maximino Bessa: Universidade de Trás-os-Montes e Alto Douro
J1231,Exploring the SenseMaking Process through Interactions and fNIRS in Immersive Visualization,,,"Theories of cognition inform our decisions when designing human-computer interfaces, and immersive systems enable us to examine these theories. This work explores the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem. We assessed how the interface layout and the task challenge level influenced users' interactions, how these interactions changed over time, and how they influenced task performance. We found that increased interactions and cerebral hemodynamic responses were associated with more accurate performance, especially on cognitively demanding trials, and discuss about sensemaking from the perspective of embodied and distributed cognition.",Alexia Galati: University of North Carolina at Charlotte; Riley J Schoppa: UNC Charlotte; Aidong Lu: University of North Carolina at Charlotte
J1244,"DeProCams: Simultaneous Relighting, Compensation and Shape Reconstruction for Projector-Camera Systems",,,"Image-based relighting, projector compensation and depth/normal reconstruction are three important tasks of projector-camera systems (ProCams) and spatial augmented reality (SAR). Although they share a similar pipeline of finding projector-camera image mappings, in tradition, they are addressed independently, sometimes with different prerequisites, devices and sampling images. We propose a novel end-to-end trainable model named DeProCams to explicitly learn the photometric and geometric mappings of ProCams, and once trained, DeProCams can be applied simultaneously to the three tasks. By solving the three tasks in a unified model, DeProCams waives the need for additional optical devices, radiometric calibrations, and structured light.",Bingyao Huang: Stony Brook University; Haibin Ling: Stony Brook University
J1254,LiveObj: Object Semantics-based Viewport Prediction for Live Mobile Virtual Reality Streaming,,,"Virtual reality (VR) video streaming has been gaining popularity recently as a new form of multimedia providing the users with immersive viewing experience. However, the high volume of data for the 360-degree video frames creates significant bandwidth challenges. We develop a live viewport prediction mechanism, namely LiveObj, by detecting the objects in the video based on their semantics. The detected objects are then tracked to infer the user's viewport in real time by employing a reinforcement learning algorithm. Our evaluations based on 48 users watching 10 VR videos demonstrate high prediction accuracy and significant bandwidth savings achieved by LiveObj.",Xianglong Feng: Rutgers University; Zeyang Bao: Rutgers University; Sheng Wei: Rutgers University
J1273,Instant Panoramic Texture Mapping with Semantic Object Matching for Large-Scale Urban Scene Reproduction,,,"This paper proposes a novel panoramic texture mapping-based rendering system for real-time, photorealistic reproduction of large-scale urban scenes at a street level. To provide users with free walk-through experiences in global urban streets, our system effectively covers large-scale scenes by using sparsely sampled panoramic street-view images and simplified scene models, which are easily obtainable from open databases. Our key concept is to extract semantic information from the given street-view images and to deploy it in proper intermediate steps of the suggested pipeline, which results in enhanced rendering accuracy and performance time. Furthermore, our method supports real-time semantic 3D inpainting to handle occluded and untextured areas, which appear often when the user's viewpoint dynamically changes.",Jinwoo Park: KAIST; Ikbeom Jeon: KAIST; Sung-eui Yoon: KAIST; Woontack Woo: KAIST
J1305,EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking,,,"Ellipse fitting, an essential component in video oculography, is performed on previously segmented eye parts generated using computer vision techniques. Occlusions due to eyelid shape, camera position or eyelashes, break ellipse fitting algorithms that rely on edge segments. We propose training a convolutional neural network to segment entire elliptical structures and demonstrate that such a framework is robust to occlusions and offers superior pupil and iris tracking performance (at least 10% and 24% increase in pupil and iris center detection rate within a two-pixel error margin) compared to standard eye parts segmentation for multiple publicly available synthetic segmentation datasets.",Rakshit Sunil Kothari: Rochester Institute of Technology; Aayush Kumar Chaudhary: Rochester Institute of Technology; Reynold Bailey: Rochester Institute of Technology; Jeff B Pelz: Rochester Institute of Technology; Gabriel Jacob Diaz: Rochester Institute of Technology
,,,, ,