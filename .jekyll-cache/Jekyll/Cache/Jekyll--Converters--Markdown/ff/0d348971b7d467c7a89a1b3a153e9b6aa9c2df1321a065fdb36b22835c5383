I"!V<style>
    .styled-table {
        border-collapse: collapse;
        margin: 25px 0;
        font-size: 0.8em;
        font-family: sans-serif;
        /*min-width: 400px;*/
        box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
        display: table;
    }

    .styled-table thead tr {
        background-color: #00aeef;
        color: #ffffff;
        text-align: left;
    }

    .styled-table th,
    .styled-table td {
        padding: 12px 15px;
    }

    .styled-table tbody tr {
        border-bottom: 1px solid #dddddd;
    }

    .styled-table tbody tr:nth-of-type(even) {
        background-color: #f3f3f3;
    }

    .styled-table tbody tr:last-of-type {
        border-bottom: 2px solid #00aeef;
    }

    .styled-table tbody tr.active-row {
        font-weight: bold;
        color: #00aeef;
    }

    input[type='checkbox'] {
        display: none;
    }

    .wrap-collabsible {
        margin: 1rem 0;
    }

    .lbl-toggle {
        display: block;
        font-weight: bold;
        /* font-family: monospace; */
        font-size: 1rem;
        text-align: left;
        padding: 0.1rem;
        color: #00aeef;
        background: #ffffff;
        cursor: pointer;
        border-radius: 7px;
        transition: all 0.25s ease-out;
    }

    .lbl-toggle:hover {
        /*color: #FFF;*/
    }

    .lbl-toggle::before {
        content: ' ';
        display: inline-block;
        border-top: 5px solid transparent;
        border-bottom: 5px solid transparent;
        border-left: 5px solid currentColor;
        vertical-align: middle;
        margin-right: .7rem;
        transform: translateY(-2px);
        transition: transform .2s ease-out;
    }

    .toggle:checked+.lbl-toggle::before {
        transform: rotate(90deg) translateX(-3px);
    }

    .collapsible-content {
        max-height: 0px;
        overflow: hidden;
        transition: max-height .25s ease-in-out;
    }

    .toggle:checked+.lbl-toggle+.collapsible-content {
        max-height: 1500px;
    }

    .toggle:checked+.lbl-toggle {
        border-bottom-right-radius: 0;
        border-bottom-left-radius: 0;
    }

    .collapsible-content .content-inner {
        background: white;
        /* rgba(0, 105, 255, .2);*/
        border-bottom: 1px solid white;
        border-bottom-left-radius: 7px;
        border-bottom-right-radius: 7px;
        padding: .5rem 1rem;
    }

    .collapsible-content p {
        margin-bottom: 0;
    }

    /* video container */
    .video-container {
        overflow: hidden;
        position: relative;
        width: 100%;
    }

    .video-container::after {
        padding-top: 56.25%;
        /* 75% if 4:3*/
        display: block;
        content: '';
    }

    .video-container iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }

</style>

<div>
    <table class="styled-table">

        <tr>
            <th>Videos</th>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1001">MyChanges: Tools for the co-designing of housing transformations</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1002">A Collaborative VR Murder Mystery using Photorealistic User Representations</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1003">Shared Augmented Reality Experience Between a Microsoft Flight Simulator User and a User in the Real World</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1004">Collaborative VR for Liver Surgery Planning using Wearable Data Gloves: An Interactive Demonstration</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1005">A Seamless Natural Locomotion Concept for VR adventure game ”The Amusement”</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1008">The importance of sensory feedback to enhance embodiment during virtual training of myoelectric prostheses users</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1009">Visualizing Planetary Spectroscopy through Immersive On-Site Rendering</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1011">Circuit World: A Multiplayer VE for Researching Engineering Learning</a></td>
        </tr>
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#V1013">Swing</a></td>
        </tr>
        
    </table>
</div>

<div>
    

    <h3 id="V1001">MyChanges: Tools for the co-designing of housing transformations</h3>
    <p><i>Sara Eloy, Micaela Raposo, Fábio Costa, Pieter E. Vermaas</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/iWtqOEc4wLc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1001" class="wrap-collabsible"> <input id="collapsibleV1001" class="toggle" type="checkbox" /> <label for="collapsibleV1001" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>MyChanges is a prototype tool for generating and visualizing architectonic modifications of existing housing in co-design projects with inhabitants. Our hypothesis is that giving inhabitants design solutions that would fit individual needs and aspirations, will increase their satisfaction with their house. To arrive at architectonically responsible house transformations, we used a shape grammar system for defining the possible modifications. For empowering inhabitants to understand and explore these modifications to their housing and increase the potential of their participation, we developed a mockup tool that comprehends two main parts: shape generation and visualization. The shape generation component is currently a mockup simulation that reproduces some of the generation possibilities of the grammar. For visualizing the outcomes, we developed three different possibilities: i) a semi-immersive visualization where the user utilizes a smartphone to see a 360º render of the site; ii) a fully immersive visualization developed with Unity in which the user with a Head Mounted Display, can freely navigate through the final design; iii) a non-immersive screen-based visualization, where the user, with a tablet device, visualizes a static image of the final design. Interviews and tests with real inhabitants (n=12) were performed to assess the user’s response to the potential of the tools and preliminary conclusions show that a tool like MyChanges would have acceptance among inhabitants.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1002">A Collaborative VR Murder Mystery using Photorealistic User Representations</h3>
    <p><i>Ana Revilla, Sergio Zamarvide, Ignacio Lacosta, Fernando Perez, Javier Lajara, Bart Kevelham, Valérie Juillard, Brian Rochat, Michelle Drocco, Natasha Devaud, Olivier Barbeau, Caecilia Charbonnier, , Patrick de Lange, Jie Li, Yanni Mei, Kinga Ławicka, Jack Jansen, Ignacio Reimat, Shishir Subramanyam, Pablo Cesar</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/mC9U3EJ83UQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1002" class="wrap-collabsible"> <input id="collapsibleV1002" class="toggle" type="checkbox" /> <label for="collapsibleV1002" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The VRTogether project has developed a Social VR platform for remote communication and collaboration. The hyper-realistic representation of users, as volumetric video, allows for natural interaction in a virtual environment with others. This video shows one of the use cases, an escape room style, where remote users need to collaboratively resolve a murder mystery. The experience takes place in the victim’s apartment where the police team (avatars) together with up to four real-time captured users (point clouds), work as a team to find clues and come up with a conclusion about what happened to the victim and who was the criminal. This experience includes a layer of interaction, enabling the users to interact with the environment, by touching objects, and to talk to the characters. It also allows for navigating between the rooms of the apartment. The experience provides immersion and social connectedness, where users are protagonists of the story, sharing the virtual environment and following the narrative. The combination of virtual reality environments (space and characters) with novel technologies for real-time volumetric video conferencing enables unique new experiences in a number of areas such as healthcare, broadcasting, and gaming.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1003">Shared Augmented Reality Experience Between a Microsoft Flight Simulator User and a User in the Real World</h3>
    <p><i>Christoph Leuze, Matthias Leuze</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/ngPJNtdsviU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1003" class="wrap-collabsible"> <input id="collapsibleV1003" class="toggle" type="checkbox" /> <label for="collapsibleV1003" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Our video shows an application where a user with an augmented reality (AR) display watches another user, flying an airplane in the Microsoft Flight Simulator 2020 (MSFS), at their respective location in the real world. To do that, we take the location data of a virtual 3D airplane model in a virtual representation of the world of a user playing MSFS, and stream it via a server to a mobile AR device. The mobile device user can then see the same 3D airplane model at exactly that real world location, that corresponds to the location of the virtual 3D airplane model in the virtual representation of the world. The mobile device user can also see the plane movement updated according to the 3D airplane movement in the virtual world. We implemented the application on both a cellphone and the Hololens 2.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1004">Collaborative VR for Liver Surgery Planning using Wearable Data Gloves: An Interactive Demonstration</h3>
    <p><i>Vuthea Chheang, Vikram Apilla, Patrick Saalfeld, Christian Boedecker, Tobias Huber, Florentine Huettl, Hauke Lang, Bernhard Preim, Christian Hansen</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/AOWCVn2iRFY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1004" class="wrap-collabsible"> <input id="collapsibleV1004" class="toggle" type="checkbox" /> <label for="collapsibleV1004" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Pre-operative planning for liver surgery is a critical procedure to assess a potential resection and it supports surgeons to define the affected vessels and resection volume. Traditional surgical planning systems are widely used to support the planning with the usage of desktop-based 3D and 2D visualizations. However, desktop-based systems offer limited interactions and visualizations compared to virtual reality (VR) [3]. A suitable technique to support collaboration among surgeons is required. Our previous works [1, 2] illustrate that using collaborative VR is essential to enhance communication, teamwork, and over-distance collaboration. In this work, we present a collaborative VR prototype to support liver surgery planning with intuitive interactions using wearable data gloves and VR controllers. The users can explore the patient data in both 2D and 3D representations. Thereafter, a virtual resection is specified by drawing lines on the 3D model representation. The virtual resection is further refined to keep safety margins from the tumors. Moreover, real-time risk maps are visualized to support the surgeons during the modification. Finally, the results of the virtual resection are visualized as resection volumes with their indicated amount and colors. Future work aims to conduct an extensive study to compare the suitability of these input devices.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1005">A Seamless Natural Locomotion Concept for VR adventure game ”The Amusement”</h3>
    <p><i>Marc Barnes, Dennis Briddigkeit, Tim Mayer, Hannah Paulmann, Eike Langbehn</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/N0lih0dGg0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1005" class="wrap-collabsible"> <input id="collapsibleV1005" class="toggle" type="checkbox" /> <label for="collapsibleV1005" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The Amusement is a story-driven VR adventure about an old abandoned amusement park in the 1920s. The main game mechanics are exploration and discovery. Therefore, one of the most unique points of this game is the locomotion concept. It was designed with the following goal: The whole park can be explored entirely with natural locomotion techniques to create a seamless fusion of exploration and discovery. To achieve this, The Amusement leverages a mix of different techniques: 1) Redirected Walking techniques [1] such as rotation gains [2] and impossible spaces [3], 2) Passive movement: lifts, platforms, vehicles, etc., 3) Indirect movement: for example climbing, pulling a rope to move a raft, moving a handle of a railroad trolley, etc. Furthermore, techniques are extended by or coupled with physical interactions or movements like crawling, jumping or sliding around corners. A seamless integration of all these techniques enables the player to walk around freely in the recommended room-scale play space of 2X2 meters (or 6.6x6.6 feet). All the while they are under the impression they are exploring a virtual world as large as we want it to be.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1008">The importance of sensory feedback to enhance embodiment during virtual training of myoelectric prostheses users</h3>
    <p><i>Reidner Santos Cavalcante, Aya Gaballa, John Cabibihan, Alcimar Soares, Edgard Afonso Lamounier Jr.</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/ssAELKy7jeA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1008" class="wrap-collabsible"> <input id="collapsibleV1008" class="toggle" type="checkbox" /> <label for="collapsibleV1008" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Amputation is a lasting life experience. Furthermore, prosthesis fitting and training are usually long and sometimes frustrating experiences. Few approaches explore the problem of controlling virtual myoelectric prostheses, since integration with current VR devices is not a trivial issue. Besides, the lack of proper sensory feedback often leads to an experience far too 'artificial' for most users. In this work, we propose a system that uses Immersive Virtual Reality (IVR) and EMG signal processing to provide a training environment for amputees who are supposed to use myoelectric prostheses. Here, we also investigate the efficiency of learning how to control a virtual prosthesis with and without sensory feedback. The system runs on a Vive Pro System using a Vive Tracker to track the user's arm position and orientation and mirror the movement into a virtual prosthesis. EMG signals control the opening and closing of a virtual prosthesis. Vibrational elements placed on the user's forearm provide sensory feedback. Our results show that virtual training can be significantly improved when proper tactile feedback is provided, especially for myoelectric controlled system. EMG control and tactile feedback have provided an effective control loop, which allows for a better sense of realism for the training protocol.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1009">Visualizing Planetary Spectroscopy through Immersive On-Site Rendering</h3>
    <p><i>Lauren Gold, Alireza Bahremand, Connor Richards, Justin Hertzberg, Kyle Sese, Alexander A. Gonzalez, Zoe Purcell, Kathryn E. Powell, Robert LiKamWa</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/Wz3Nzo09qko" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1009" class="wrap-collabsible"> <input id="collapsibleV1009" class="toggle" type="checkbox" /> <label for="collapsibleV1009" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Remote sensing is currently the primary method of obtaining knowledge about the composition and physical properties of the surface of other planets. In a commonly used technique, visible and near infrared (VNIR) spectrometers onboard orbiting satellites capture reflectance data at different wavelengths, which in turn gives insight about the minerals present and the overall composition of the terrain. In select locations on Mars, rovers have also conducted up close in-situ investigation of the same terrains examined by orbiters, allowing direct comparisons at different spatial scales. In this work, we build Planetary Prism, a virtual reality tool to visualize orbital and ground data around NASA’s Mars Science Laboratory Curiosity rover’s ongoing traverse in Gale Crater. We have built a 3D terrain along Curiosity’s traverse using rover images, and within it we visualize satellite data as polyhedrons, superimposed on the terrain. This system provides perspectives of VNIR spectroscopic data from a satellite aligned with ground images from the rover, allowing the user to explore both the physical aspects of the terrain and their relation to the mineral composition. The result is a system that provides seamless rendering of data sets at vastly different scales.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1011">Circuit World: A Multiplayer VE for Researching Engineering Learning</h3>
    <p><i>Stephen B. Gilbert</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/qKsjmnVvnWI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1011" class="wrap-collabsible"> <input id="collapsibleV1011" class="toggle" type="checkbox" /> <label for="collapsibleV1011" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Business travel leads to the consumption of large amounts of energy. Circuit World (CW), a Unity virtual environment, uses avatars and realistic simulations of electrical circuits to test the effectiveness of teaching complex engineering tasks without travel. To evaluate CW, a study compares how well participants learn to repair a circuit when trained face-to-face, via Zoom, and within CW wearing an HMD. After training and completion of a knowledge quiz, learners are given a circuit to repair by themselves using a non-immersive desktop version of CW. Participants repair it again two weeks later to test retention Each study session involves three people: learner, trainer, and researcher. The learner and trainer control avatars and the researcher, invisible, observes over the learner’s shoulder. Collected data includes task performance logs from CW, knowledge quiz scores, and, based on video recordings, eye gaze tracking, behavioral coding, and measures of facial synchrony between trainer and learner.</p>
            </div>
        </div>
    </div>

    

    <h3 id="V1013">Swing</h3>
    <p><i>Mari Jaye, Mark Reisch, Vicky Mejia Yepes</i></p>

    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/aysKaOpFa2w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>
    
    <div id="V1013" class="wrap-collabsible"> <input id="collapsibleV1013" class="toggle" type="checkbox" /> <label for="collapsibleV1013" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Swing is a narrative film in virtual reality space combining 2D, 3D and 2.5D animation techniques. The story unfolds in overlapping acts that depict the internal and external struggles of a frustrated girl who is attempting and failing to swing on a playground swing. As she struggles with the swing, she imagines the demise of the children swinging successfully around her, and anger builds until she ends up taking it out on herself. When she finally hits rock bottom, she lets go, leans back and swings. A new perspective is found.</p>
            </div>
        </div>
    </div>

    
</div>
:ET